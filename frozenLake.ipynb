{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "source": [
    "## Creating The Environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "source": [
    "## Creating the Q-Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "actionSpaceSize = env.action_space.n\n",
    "stateSpaceSize = env.observation_space.n\n",
    "\n",
    "qTable = np.zeros((stateSpaceSize, actionSpaceSize))\n",
    "\n",
    "print(qTable)"
   ]
  },
  {
   "source": [
    "## Initializing Q-Learning Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpisodes = 10000\n",
    "maxStepsPerEpisode = 100\n",
    "\n",
    "learningRate = 0.1\n",
    "discountRate = 0.99\n",
    "\n",
    "explorationRate = 1\n",
    "maxExplorationRate = 1\n",
    "minExplorationRate = 0.01\n",
    "explorationDecayRate = 0.01"
   ]
  },
  {
   "source": [
    "## Q-Learning Algorithm Training Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "********* Average reward per thousand episodes *********\n\n1000: 0.5280000000000004\n2000: 0.6740000000000005\n3000: 0.6900000000000005\n4000: 0.7060000000000005\n5000: 0.6630000000000005\n6000: 0.6780000000000005\n7000: 0.6780000000000005\n8000: 0.6530000000000005\n9000: 0.6550000000000005\n10000: 0.6860000000000005\n"
     ]
    }
   ],
   "source": [
    "rewardsAllEpisodes = []\n",
    "\n",
    "# Q-Learning Algorithm\n",
    "for episode in range(numEpisodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewardsCurrentEpisode = 0\n",
    "\n",
    "    for step in range(maxStepsPerEpisode):\n",
    "        # Exploration-exploitation trade-off\n",
    "        explorationRateThreshold = random.uniform(0, 1)\n",
    "        if (explorationRateThreshold > explorationRate):\n",
    "            action = np.argmax(qTable[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take new action\n",
    "        newState, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        qTable[state, action] = qTable[state, action] * (1 - learningRate) + learningRate * (reward + discountRate * np.max(qTable[newState, :]))\n",
    "\n",
    "        # Set new state\n",
    "        state = newState\n",
    "        rewardsCurrentEpisode += reward\n",
    "\n",
    "        if (done == True):\n",
    "            break\n",
    "        \n",
    "    # Exploration rate decay\n",
    "    explorationRate = minExplorationRate + (maxExplorationRate - minExplorationRate) * np.exp(-explorationDecayRate * episode)\n",
    "\n",
    "    # Add current episode reward to total rewards list\n",
    "    rewardsAllEpisodes.append(rewardsCurrentEpisode)\n",
    "\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewardsPerThousandEpisodes = np.split(np.array(rewardsAllEpisodes), numEpisodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********* Average reward per thousand episodes *********\\n\")\n",
    "\n",
    "for r in rewardsPerThousandEpisodes:\n",
    "    print(f\"{count}: {str(sum(r/1000))}\")\n",
    "    count += 1000"
   ]
  },
  {
   "source": [
    "## Watch the Agent Play the Game"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "****You fell through the hole!****\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(f\"***** EPISODE {episode + 1} *****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(maxStepsPerEpisode):\n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        # Choose action with highest Q-value for current state\n",
    "        action = np.argmax(qTable[state, :])\n",
    "        # Take new action\n",
    "        newState, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            if (reward == 1):\n",
    "                # agent reached goal and won episode\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode\n",
    "                print(\"****You fell through the hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        # set new state\n",
    "        state = newState\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}